# Week 1 Presentation Script (æ¼”è®²ç¨¿)
## 5-7åˆ†é’Ÿå£å¤´æ¼”è®²è„šæœ¬

---

## Opening (30ç§’)

"Good afternoon everyone. I'm Kelin Wu, and today I'm presenting my EE542 final project: **Smart Plant Care System using Deep Reinforcement Learning**.

This project addresses a $12 billion dollar problem in smart agriculture: **50% of irrigation water is wasted** globally, yet 40% of home plants still die within their first year.

Why? Because traditional systems use fixed schedules that ignore real-time plant needs."

---

## Problem & Market (1åˆ†é’Ÿ)

"Let me show you why this matters commercially.

[Point to Slide 2]

Three profitable companies are already in this space:
- **John Deere**, with $52 billion in revenue, uses AI for precision agriculture
- **Click & Grow**, valued at over $50 million, makes smart indoor planters
- **AeroGarden** generates over $100 million annually in home hydroponic systems

But here's the issue: their current solutions still rely on **fixed timers** or **simple threshold rules**. They're not truly intelligent.

User research shows:
- Home gardeners complain: 'I water on schedule but my plants still die'
- Vertical farm operators say: 'We waste 30% of water but crops still experience stress'

There's clearly a gap for **adaptive AI-driven care**."

---

## Technical Approach (1.5åˆ†é’Ÿ)

"So how do I solve this? Through **Advanced Reinforcement Learning** - specifically, the Proximal Policy Optimization algorithm.

[Point to Slide 3]

This falls under **Category 1** of the course requirements: Advanced RL.

Here's how it works:

**State Space**: The AI observes 6 things every hour:
- Soil moisture level
- Temperature
- Light intensity
- Time of day
- Plant health score
- Hours since last watering

**Action Space**: It decides two things:
- How much water to add (0 to 100 milliliters)
- Whether to turn the grow lamp on or off

**Reward Function**: The AI is rewarded for:
- Increasing plant health *(alpha term)*
- While minimizing water usage *(beta penalty)*
- And energy consumption *(gamma penalty)*
- And avoiding constraint violations *(delta penalty)*

This is a true RL problem because:
1. The agent learns through **trial and error** - no labeled data needed
2. It handles **delayed rewards** - watering effects appear hours later
3. It does **multi-objective optimization** - balancing health and efficiency

No simple rule-based system can do this."

---

## GPU Acceleration (45ç§’)

"One of the course requirements is GPU acceleration, and this project is perfect for it.

[Point to Slide 4]

PPO training is **compute-intensive** because it continuously updates neural network policies while running multiple parallel environments.

On CPU, training takes 24 hours.  
With an **RTX 5090 GPU**, it drops to **3 hours** - that's **8 times faster**.

I'll demonstrate GPU utilization in TensorBoard during training, showing over 80% GPU usage throughout."

---

## Week 1 Achievements (1åˆ†é’Ÿ)

"Now let me show you what I've actually built this week.

[Point to Slide 5]

**First**, I implemented the complete environment:
- A `PlantPhysics` class that models soil moisture dynamics, photosynthesis using Michaelis-Menten kinetics, and stress responses
- A `PlantCareEnv` that's fully compatible with OpenAI Gym standard
- It simulates 30 days of plant growth in realistic time

**Second**, I built two baseline strategies:
- A **Fixed Schedule** that waters at 8am and 8pm daily
- A **Threshold Rule** that waters when moisture drops below 30%

**Third**, evaluation framework:
- Metrics tracking for health, water usage, energy, constraint violations
- Visualization tools that generate comparison charts

All of this is **fully functional and tested**."

---

## Results Demo (1åˆ†é’Ÿ)

"Let me show you the baseline results.

[Point to Slide 6]

I ran both baseline strategies for 30-day simulations, 5 episodes each:

**Fixed Schedule**: 
- Average health: 60 out of 100
- Uses 10.2 liters of water per month
- Violates constraints (too dry or too wet) for 120 hours

**Threshold Rule**: 
- Slightly better: health of 70
- Uses 8.5 liters
- 80 violation hours

**But my PPO target** is:
- Health of **87** - that's **45% improvement**
- Only **5.8 liters** - that's **43% water savings**
- Just **35 violation hours** - **71% reduction**

[Optional: Show visualization if time permits]

This demonstrates that even semi-smart rules waste resources. We need true adaptive intelligence."

---

## Next Steps (45ç§’)

"Looking ahead to Weeks 2 and 3:

[Point to Slide 9]

**Week 2**: I'll integrate the Stable-Baselines3 library, configure PPO hyperparameters, and run initial training tests.

**Week 3**: Full GPU training - 5 million steps taking about 3 hours. I'll tune hyperparameters and generate the final performance comparisons.

The goal is a trained PPO model that conclusively beats both baselines across all metrics."

---

## Risk Management (30ç§’)

"Of course, there are risks.

[Point to Slide 11]

The main one is **PPO not converging**. If that happens, I have backup plans:
- Adjust hyperparameters following Stable-Baselines3 documentation
- Simplify the state space by removing less important features
- Lower the target to 80 health instead of 87 - still a 33% improvement

For **GPU availability**, I can use Google Colab or AWS spot instances as backups.

I'm confident the project is achievable within 6 weeks."

---

## Closing (30ç§’)

"To summarize:

This project solves a **real $12 billion problem** in smart agriculture.

It uses **Category 1 Advanced Reinforcement Learning** with PPO.

It **requires GPU acceleration** for efficient training.

**Week 1 is complete** - environment and baselines are working.

And the expected outcome is **45% better plant health with 43% less water**.

Thank you! I'm happy to take questions."

---

## Q&A Preparation (å¸¸è§é—®é¢˜å›ç­”)

### Q: "Why not just use a PID controller?"
"Great question. PID controllers need manual tuning for each plant type, and they don't handle multi-objective optimization well. RL learns automatically and can balance health, water, and energy simultaneously. Plus, RL can discover non-intuitive strategies that humans might miss."

### Q: "How do you validate without real plants?"
"The simulation is based on peer-reviewed plant physiology research - specifically soil moisture dynamics and photosynthesis models from Thornley and Johnson's textbook. While it's a simulation, the physics are grounded in real science. Future work would involve transfer learning to real hardware, but for this course project, simulation is sufficient to demonstrate the RL concepts."

### Q: "What if your PPO model doesn't outperform the baselines?"
"If that happens, it means my reward function or hyperparameters need tuning. I've built in flexibility - I can adjust the reward weights, change network architecture, or simplify the state space. Worst case, I demonstrate that RL is competitive with baselines while using less human engineering effort."

### Q: "How many interviews did you do?"
"As an individual project, I need 2 interviews minimum. I focused on one home gardener to understand consumer pain points, and one industry professional to validate the commercial angle. I supplemented this with public research data from industry reports and online community analysis."

### Q: "Can this actually deploy to real hardware?"
"Yes, with some engineering work. The trained policy is just a neural network - it can run on a Raspberry Pi. You'd need real sensors (moisture, temperature, light) and actuators (water pump, lamp relay). The main challenges would be sensor calibration and handling actuator failures, but those are engineering problems, not fundamental barriers."

---

## Time Management Tips

- **Total: 5-7 minutes**
- **Slides 1-6**: Core content (4-5 min)
- **Slides 7-10**: Technical depth (1-2 min)
- **Slides 11-15**: Optional (skip if running short)
- **Demo**: Only if time permits and works smoothly

**Pro tip**: Practice with a timer! Aim for 5 minutes to leave room for questions.

---

## Presentation Delivery Tips

### å£°éŸ³å’Œå§¿æ€
1. **Speak clearly** and at moderate pace (not too fast)
2. **Make eye contact** with audience
3. **Use hand gestures** when explaining architecture
4. **Show enthusiasm** - this is a cool project!

### æŠ€æœ¯æ¼”ç¤º
1. **Pre-record backup video** in case live demo fails
2. **Test everything** 30 minutes before presentation
3. **Have terminal ready** with commands pre-typed
4. **Don't panic if demo fails** - show recorded version

### å¹»ç¯ç‰‡æŠ€å·§
1. **Don't read slides** - use them as visual aids
2. **Point to specific charts/numbers** when discussing
3. **Advance slides smoothly** - practice timing
4. **Have backup laptop** or slides on USB

### é—®ç­”ç¯èŠ‚
1. **Repeat the question** before answering (for clarity)
2. **Be honest** if you don't know - "That's a great question I'll explore"
3. **Keep answers concise** - 30-60 seconds max
4. **Redirect** if question is off-topic: "Interesting, but out of scope for this course"

---

Good luck! ğŸš€










